{"task_id": "python_1", "code": "import sys\n\nifunc, g = lambda: [*map(int, sys.stdin.readline().rstrip().split())], range\n\nR, C, T = ifunc()\nbo = [ifunc() for _ in g(R)]\nupAir, downAir = [(p//C, p%C) for p in g(R*C) if bo[p//C][p%C] == -1]\n\n\ndef move(spos, epos, inVal):\n    global bo\n\n    outVal = bo[epos[0]][epos[1]]\n\n    dpos = (epos[0]-spos[0], epos[1]-spos[1])\n    dposSize = sum([*map(abs, dpos)])\n    dpos = tuple(item//dposSize\n     for item in dpos)\n\n    spos = (spos[0]+dpos[0], spos[1]+dpos[1])\n    cpos = epos\n    while spos != cpos:\n        bpos = (cpos[0]-dpos[0], cpos[1]-dpos[1])\n        bo[cpos[0]][cpos[1]] = bo[bpos[0]][bpos[1]]\n        cpos = bpos\n    bo[cpos[0]][cpos[1]] = inVal\n\n    return outVal\n\ndef moveCircle(*pList):\n    global bo\n\n    sPos = pList[0]\n    outVal = 0\n    for ePos in pList[1:]:\n        outVal = move(sPos, ePos, outVal)\n        sPos = ePos\n    move(sPos, pList[0], outVal)\n    \n    bo[pList[0][0]][pList[0][1]] = -1\n    \n\ndef diffuse():\n    global bo\n\n    bo2 = [[0 for _ in g(C)] for _ in g(R)]\n\n    for r in g(R): \n        for c in g(C):\n            if bo[r][c] in [0, -1]: continue\n            amount = bo[r][c]//5\n            for nr, nc in ((r+1, c), (r, c+1), (r-1, c), (r, c-1)):\n                if nr < 0 or nr >= R or nc < 0 or nc >= C: continue\n                if bo[nr][nc] == -1: continue\n                bo[r][c] -= amount\n                bo2[nr][nc] += amount\n            bo2[r][c] += bo[r][c]\n\n    bo2[upAir[0]][upAir[1]] = -1\n    bo2[downAir[0]][downAir[1]] = -1\n\n    bo = bo2\n", "entry_point": "diffuse"}
{"task_id": "python_2", "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\nimport math\n\n\ndef ucb():\n    dataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n    N = 10000\n    d = 10\n    ads_selected = []\n    numbers_of_selections = [0] * d\n    sums_of_rewards = [0] * d\n    total_reward = 0\n\n    for n in range(0, N):\n        ad = 0\n        max_upper_bound = 0\n\n        for i in range(0, d):\n            if (numbers_of_selections[i] > 0):\n                average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n                delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n                upper_bound = average_reward + delta_i\n            else:\n                upper_bound = 1e400\n            if (upper_bound > max_upper_bound):\n                max_upper_bound = upper_bound\n                ad = i\n\n    ads_selected.append(ad)\n    numbers_of_selections[ad] = numbers_of_selections[ad] + 1\n    reward = dataset.values[n, ad]\n    sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n    total_reward = total_reward + reward\n    return ads_selected, total_reward\n\n\ndef plt_hist(ads_selected):\n    plt.hist(ads_selected)\n    plt.title('Histogram of ads selections')\n    plt.xlabel('Ads')\n    plt.ylabel('Number of times each ad was selected')\n    plt.show()\n\ndef main():\n    ads_selected, total_reward = ucb()\n    plt_hist(ads_selected)\n", "entry_point": "ucb"}
{"task_id": "python_3", "code": "\nfrom sympy.combinatorics import Permutation\nfrom sympy.combinatorics.util import _distribute_gens_by_base\n\nrmul = Permutation.rmul\n\n\ndef _cmp_perm_lists(first, second):\n\n    return {tuple(a) for a in first} == \\\n           {tuple(a) for a in second}\n\n\ndef _naive_list_centralizer(self, other, af=False):\n    from sympy.combinatorics.perm_groups import PermutationGroup\n\n    from sympy.combinatorics.permutations import _af_commutes_with\n    if hasattr(other, 'generators'):\n        elements = list(self.generate_dimino(af=True))\n        gens = [x._array_form for x in other.generators]\n        commutes_with_gens = lambda x: all(_af_commutes_with(x, gen) for gen in gens)\n        centralizer_list = []\n        if not af:\n            for element in elements:\n                if commutes_with_gens(element):\n                    centralizer_list.append(Permutation._af_new(element))\n        else:\n            for element in elements:\n                if commutes_with_gens(element):\n                    centralizer_list.append(element)\n        return centralizer_list\n    elif hasattr(other, 'getitem'):\n        return _naive_list_centralizer(self, PermutationGroup(other), af)\n    elif hasattr(other, 'array_form'):\n        return _naive_list_centralizer(self, PermutationGroup([other]), af)\n\n\ndef canonicalize_naive(g, dummies, sym, *v):\n\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    from sympy.combinatorics.tensor_can import gens_products, dummy_sgs\n    from sympy.combinatorics.permutations import Permutation, _af_rmul\n    v1 = []\n    for i in range(len(v)):\n        base_i, gens_i, n_i, sym_i = v[i]\n        v1.append((base_i, gens_i, [[]]*n_i, sym_i))\n    size, sbase, sgens = gens_products(*v1)\n    dgens = dummy_sgs(dummies, sym, size-2)\n    if isinstance(sym, int):\n        num_types = 1\n        dummies = [dummies]\n        sym = [sym]\n    else:\n        num_types = len(sym)\n    dgens = []\n    for i in range(num_types):\n        dgens.extend(dummy_sgs(dummies[i], sym[i], size - 2))\n    S = PermutationGroup(sgens)\n    D = PermutationGroup([Permutation(x) for x in dgens])\n    dlist = list(D.generate(af=True))\n    g = g.array_form\n    st = set()\n    for s in S.generate(af=True):\n        h = _af_rmul(g, s)\n        for d in dlist:\n            q = tuple(_af_rmul(d, h))\n            st.add(q)\n    a = list(st)\n    a.sort()\n    prev = (0,)*size\n    for h in a:\n        if h[:-2] == prev[:-2]:\n            if h[-1] != prev[-1]:\n                return 0\n        prev = h\n    return list(a[0])\n\n\ndef graph_certificate(gr):\n\n    from sympy.combinatorics.permutations import _af_invert\n    from sympy.combinatorics.tensor_can import get_symmetric_group_sgs, canonicalize\n    items = list(gr.items())\n    items.sort(key=lambda x: len(x[1]), reverse=True)\n    pvert = [x[0] for x in items]\n    pvert = _af_invert(pvert)\n\n    \n    num_indices = 0\n    for v, neigh in items:\n        num_indices += len(neigh)\n\n    vertices = [[] for i in items]\n    i = 0\n    for v, neigh in items:\n        for v2 in neigh:\n            if pvert[v] < pvert[v2]:\n                vertices[pvert[v]].append(i)\n                vertices[pvert[v2]].append(i+1)\n                i += 2\n    g = []\n    for v in vertices:\n        g.extend(v)\n    assert len(g) == num_indices\n    g += [num_indices, num_indices + 1]\n    size = num_indices + 2\n    assert sorted(g) == list(range(size))\n    g = Permutation(g)\n    vlen = [0]*(len(vertices[0])+1)\n    for neigh in vertices:\n        vlen[len(neigh)] += 1\n    v = []\n    for i in range(len(vlen)):\n        n = vlen[i]\n        if n:\n            base, gens = get_symmetric_group_sgs(i)\n            v.append((base, gens, n, 0))\n    v.reverse()\n    dummies = list(range(num_indices))\n    can = canonicalize(g, dummies, 0, *v)\n    return can\n", "entry_point": "canonicalize_naive"}
{"task_id": "python_4", "code": "import functools\nimport io\nimport sys\n\ndef parseInput(lines):\n    initialState = list(lines[0][15:len(lines[0])])\n    rules = []\n    for ruleLine in lines[2:]:\n        rules.append((list(ruleLine[:5]), ruleLine[9]))\n    return initialState, rules\n\ndef applyRules(state, p, rules):\n    stateLen = len(state)\n    L2  = state[p-2] if p >= 2 and p < stateLen+2 else \".\"\n    L1  = state[p-1] if p >= 1 and p < stateLen+1 else \".\"\n    C   = state[p] if p >= 0 and p < stateLen else \".\"\n    R1  = state[p+1] if p >= -1 and p < stateLen-1 else \".\"\n    R2  = state[p+2] if p >= -2 and p < stateLen-2 else \".\"\n    val = [L2, L1, C, R1, R2]\n    for rule in rules:\n        if val == rule[0]:\n            return rule[1]\n    return \".\"\n\ndef count(state, originIndex):\n    val = 0\n    for i, p in enumerate(state):\n        if p != \".\":\n           val += i - originIndex\n    return val\n\n\ndef part1(lines):\n    state, rules = parseInput(lines)\n\n    originIndex = 0\n    plantCount = count(state, originIndex)\n    print(\"[%d] %s :: %d\" % (0, \"\".join(state), plantCount))\n\n    simLength = 20\n    for n in range(1, simLength+1):\n        newState = []\n        for p in range(-5, len(state)+5):\n            newValue = applyRules(state, p, rules)\n            if newValue != \".\" or (len(newState) > 0 and p < len(state)):\n                newState.append(newValue)\n                if p < 0:\n                    originIndex += 1\n            elif p >= 0 and p <= originIndex:\n                originIndex -= 1\n        state = newState\n        plantCount = count(state, originIndex)\n        print(\"[%d] %s :: %d / %d\" % (n, \"\".join(state), plantCount, originIndex))\n    print(\"[Part1] %d\" % plantCount)\n\ndef part2(initialState, rules):\n    pass\n\ntestInput = \"initial state: \n\ndef run(inputfile):\n    lines = []\n    with open(inputfile, \"r\") as file:\n        lines = file.readlines()\n\n    part1(lines)\n    \ndef main():\n    if (len(sys.argv) != 2):\n        print(\"usage: plants [inputfile]\")\n        return\n\n    run(sys.argv[1])\n", "entry_point": "part1"}
{"task_id": "python_5", "code": "import os\nimport sys\n\nthis_file = os.path.dirname(os.path.abspath(__file__))\ngenerated_dir = os.path.abspath(os.path.join(this_file, '..', '..', 'torch', 'csrc', 'generated'))\n\nline_start = '//generic_include '\n\ntypes = [\n    'Double',\n    'Float',\n    'Half',\n    'Long',\n    'Int',\n    'Short',\n    'Char',\n    'Byte'\n]\n\ngeneric_include = '\ngenerate_include = '\n\n\ndef get_gen_path_prefix(file_name):\n    gen_name_prefix = file_name[len('torch/csrc/'):].replace('/', '_').replace('.cpp', '')\n    gen_path_prefix = os.path.join(generated_dir, gen_name_prefix)\n    return gen_path_prefix\n\n\ndef split_types_ninja(file_name, w):\n    gen_path_prefix = get_gen_path_prefix(file_name)\n    to_build = [gen_path_prefix + t + '.cpp' for t in types]\n    myself = 'tools/setup_helpers/split_types.py'\n    cmd = \"{} {} '{}'\".format(sys.executable, myself, file_name)\n    w.writer.build(\n        to_build, 'do_cmd', [file_name, myself],\n        variables={\n            'cmd': cmd,\n        })\n    return to_build\n\n\ndef split_types(file_name, ninja_global):\n    if ninja_global is not None:\n        return split_types_ninja(file_name, ninja_global)\n\n    assert file_name.startswith('torch/csrc/')\n    if not os.path.exists(generated_dir):\n        os.makedirs(generated_dir)\n\n    with open(file_name, 'r') as f:\n        lines = f.read().split('\\n')\n\n    for i, l in enumerate(lines):\n        if l.startswith(line_start):\n            args = l[len(line_start):]\n            lib_prefix, generic_file = filter(bool, args.split())\n            break\n    else:\n        raise RuntimeError(\"generic include not found\")\n\n    gen_name_prefix = file_name[len('torch/csrc/'):].replace('/', '_').replace('.cpp', '')\n    gen_path_prefix = os.path.join(generated_dir, gen_name_prefix)\n\n    prefix = '\\n'.join(lines[:i])\n    suffix = '\\n'.join(lines[i + 1:])\n\n    to_build = []\n\n    g_include = generic_include.format(lib=lib_prefix, path=generic_file)\n    for t in types:\n        t_include = generate_include.format(lib=lib_prefix, type=t)\n        gen_path = gen_path_prefix + t + '.cpp'\n        to_build.append(gen_path)\n        with open(gen_path, 'w') as f:\n            f.write(prefix + '\\n' +\n                    g_include + '\\n' +\n                    t_include + '\\n' +\n                    suffix)\n    return to_build\n\n", "entry_point": "split_types"}
{"task_id": "python_6", "code": "import xml.etree.ElementTree as ET, re\nfrom collections import defaultdict\nfrom typing import Union\n\n\nclass XMLtoDict(object):\n\n    def parse(self, xml: str):\n        return self.__to_dict(ET.fromstring(xml))\n    \n    def value_from_nest(self, pattern: str, nest: Union[dict, str]):\n        nest = nest if type(nest) is dict else self.parse(nest)\n        for k, v in nest.items():\n            match = re.search(pattern, k)\n            if match:\n                return v\n            if type(v) is dict:\n                return self.value_from_nest(pattern, v)\n\n    def __to_dict(self, t: str):\n        d = {t.tag: {} if t.attrib else None}\n        children = list(t)\n        if children:\n            dd = defaultdict(list)\n            for dc in map(self.__to_dict, children):\n                for k, v in dc.items():\n                    dd[k].append(v)\n            d = {t.tag: {\n                    k:v[0] if len(v) == 1 \\\n                        else v for k, v in dd.items()\n            }}\n        if t.attrib:\n            d[t.tag].update(('@' + k, v) for k, v in t.attrib.items())\n        if t.text:\n            text = t.text.strip()\n            if children or t.attrib:\n                if text:\n                    d[t.tag]['\n            else:\n                d[t.tag] = text\n        return d\n", "entry_point": "__to_dict"}
{"task_id": "python_7", "code": "\n\nfrom sqlalchemy.orm.exc import NoResultFound\n\n\ndef find_or_create(session, model, **kwargs):\n    \n    q = session.query(model).filter_by(**kwargs)\n    try:\n        return q.one()\n    except NoResultFound:\n        instance = model(**kwargs)\n        session.add(instance)\n        return q.one()\n\n\ndef update_instance(instance, attrs):\n    for k, v in attrs.items():\n        setattr(instance, k, v)\n\n\ndef upsert_all(session, instances, *key_attrs):\n    \n    MAX_ROWS = 200  \n    if not instances:\n        return\n    if len(instances) > MAX_ROWS:\n        upsert_all(session, instances[:MAX_ROWS], *key_attrs)\n        upsert_all(session, instances[MAX_ROWS:], *key_attrs)\n        return\n\n    assert key_attrs\n    klass = instances[0].__class__\n    instance_map = {tuple(getattr(instance, attr.key) for attr in key_attrs): instance\n                    for instance in instances}\n\n    rows = session.query(klass)\n    for i, attr in enumerate(key_attrs):\n        rows = rows.filter(getattr(klass, attr.key).in_({k[i] for k in instance_map.keys()}))\n\n    for obj in rows:\n        key = tuple(getattr(obj, attr.key) for attr in key_attrs)\n        \n        if key not in instance_map:\n            continue\n        instance = instance_map.pop(key)\n        for c in klass.__table__.columns:\n            if c.key != 'id':\n                setattr(obj, c.key, getattr(instance, c.key))\n\n    session.add_all(instance_map.values())\n\n    counts = {\"Updated\": len(instances) - len(instance_map), \"Added\": len(instance_map)}\n    for verb, count in ((k, v) for k, v in counts.items() if v):\n        print(\"%s %d %s record(s)\" % (verb, count, klass.__name__))\n", "entry_point": "upsert_all"}
{"task_id": "python_8", "code": "\nimport math\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\ndef generate_sig(inphase_idx, W, T, amp, baseline, y, idx, x_len):\n    for x in range(inphase_idx, T):\n        if x < T/2-W/2 or x > T/2+W/2:\n            y[idx] = baseline\n        else:\n            d = W/2 - math.fabs(T/2-x)\n            y[idx] = d/(W/2)*(amp+baseline)\n\n        idx += 1\n\n        if idx >= x_len:\n            break\n\n    return idx\n\n\ndef simu_rythm_sig(x_len, baseline=0, amp=1):\n\n    W = 10 \n    DR = 2 \n\n    init_phase = np.random.rand()\n\n    y = np.zeros(x_len)\n\n    idx = 0\n\n\n    W_rand = int(math.fabs(np.random.rand()*10)+W)\n    D_rand = (np.random.rand()+1)*DR\n    T_rand = int(D_rand*W_rand)\n    amp_rand = ((np.random.rand()+0.5)*amp)\n\n\n\n    \n    if init_phase > 0:\n        inphase_idx = int(T_rand*init_phase)\n        idx = generate_sig(inphase_idx, W_rand, T_rand, amp_rand, baseline, y, idx, x_len)\n\n    while idx < x_len:\n\n        W_rand = int(math.fabs(np.random.rand()*10)+W)\n        D_rand = (np.random.rand()*2+1)*DR\n        T_rand = int(D_rand*W_rand)\n        amp_rand = ((np.random.rand()+0.5)*amp)\n\n        inphase_idx = 0\n\n        \n\n        idx = generate_sig(inphase_idx, W_rand, T_rand, amp_rand, baseline, y, idx, x_len)\n\n    return y\n\ndef add_bg_gaussian_noise(sig, amp, snr):\n    sig_noisy = np.array(sig, copy=True)\n    for idx in range(len(sig)):\n        sig_noisy[idx] = sig[idx] + np.random.random()*(10**(snr/20*amp))\n\n    return sig_noisy\n\n\ndef add_transcient_noise(sig, amp, snr, noise_len, noise_baseline):\n    sig_noisy = np.array(sig, copy=True)\n    idx_start = int(np.random.rand()*len(sig))\n    noise_len = int(np.random.rand()*noise_len)\n\n    if idx_start + noise_len > len(sig):\n        idx_range = range(idx_start, len(sig)-1)\n    else:\n        idx_range = range(idx_start, idx_start+noise_len-1)\n\n    for idx in idx_range:\n        sig_noisy[idx] += np.random.random()*(10**(snr/20*amp))*noise_baseline\n\n    return sig_noisy\n\n\n", "entry_point": "generate_sig"}
{"task_id": "python_9", "code": "\nimport argparse\nfrom intervaltree import Interval, IntervalTree\nimport math\nimport multiprocessing\nimport numpy as np\nimport sys\nimport time\n\n\ndef sliding_window_throughput(ts_data, ticks, window_size):\n\n    assert(window_size > 1)\n    count_ticks = ticks.shape[0]\n    mbps_wnd = np.zeros(count_ticks)\n    pps = np.zeros(count_ticks)\n    istart = ts_data.begin()\n    iend = ts_data.end()\n    for i, wnd_start in enumerate(ticks):\n        if wnd_start > iend:\n            break\n        wnd_end = wnd_start + window_size\n        if wnd_end < istart:\n            continue\n        intervals = ts_data.overlap(wnd_start, wnd_end)\n        for interval in intervals:\n            s = interval[0]\n            e = interval[1]\n            if s >= wnd_start and e <= wnd_end:\n                overlap = e - s\n            elif e <= wnd_end:\n                overlap = e - wnd_start\n            elif s >= wnd_start:\n                overlap =  wnd_end - s\n            else:\n                overlap = window_size\n            \n            tmp = interval[2] * 8 * overlap / (e - s)\n            assert (overlap / (e-s)) <= 1 , '{} / {}'.format(overlap, (e-s))\n            mbps_wnd[i] += tmp\n            pps[i] += 1\n        mbps_wnd[i] /= window_size\n        pps[i] /= window_size\n    return mbps_wnd, pps\n\n\n\ndef pool_fn(x):\n    return sliding_window_throughput(x[0], x[1], x[2])\n\n\ndef sec_to_us(s):\n    return int(s * 1000000)\n\n\ndef load_nginx(path_to_trace):\n    tree = IntervalTree()\n    data = np.loadtxt(path_to_trace, dtype='i8')\n    offset = data[0][0]\n    print(\"Warning: converting ns to us\")\n    for i in data:\n        \n        start = i[0] - offset\n        end = start + i[1]\n        size = int(i[2])\n\n        \n        start = start // 1000\n        end = end // 1000\n        tree.addi(start, end, size)\n    return tree\n\n\ndef load_mem(path_to_trace):\n    tree = IntervalTree()\n    with open(path_to_trace, 'r') as f:\n        for line in f:\n            raw = line.split()\n            latency = int(math.ceil(float(raw[2])))\n            end = sec_to_us(float(raw[1]))\n            start = end - latency\n            size = int(raw[4][7:-1])\n            tree.addi(start, end, size)\n    return tree\n\n\ndef load_tree(file_path, trace_type):\n    if trace_type  == \"NGINX\":\n        return load_nginx(file_path)\n    elif trace_type == \"MEMCACHED\":\n        return load_mem(file_path)\n    else:\n        raise Exception(\"Unknown trace type\")\n", "entry_point": "sliding_window_throughput"}
{"task_id": "python_10", "code": "import requests\nimport json\nimport time\nimport datetime\n\nclass GerritAPI:\n    def __init__(self, url):\n        self.url = url\n\n    \n    def fetch(self, from_date):\n        before_date = (datetime.datetime.today() + datetime.timedelta(days=1)).isoformat().replace('T','+')\n        while before_date[:10] > from_date.strftime(\"%Y-%m-%d\"):\n            print(before_date)\n            print(self.url+\"changes/?q=after:\"+from_date.strftime(\"%Y-%m-%d\")+\"+AND+before:\"+before_date[:10]+\"+AND+status:merged\")\n            response = requests.get(self.url+\"changes/?q=after:\"+from_date.strftime(\"%Y-%m-%d\")+\"+AND+before:\"+before_date[:10]+\"+AND+status:merged\")\n            print(response.ok)\n            if(response.ok):\n                data = json.loads(response.text.lstrip(\")]}\\'\\n\"))\n                if not data:\n                    break\n                before_date = (datetime.datetime.strptime(data[-1]['updated'][:10], '%Y-%m-%d') + datetime.timedelta(days=1)).isoformat().replace('T','+')\n                for change in data:\n                    change['comments'] = []\n                    comments = requests.get(self.url+\"changes/\"+change['id']+\"/comments\")\n                    if (not comments.ok):\n                        before_date = change['updated'].replace(' ', '+')\n                        break\n\n                    try:\n                        comments = json.loads(comments.text.lstrip(\")]}\\'\\n\"))\n                    except:\n                        print(\"Duplicate ids\")\n                        continue\n\n                    reviews = requests.get(self.url+\"changes/\"+change['id']+\"/detail\")\n                    if (not reviews.ok):\n                        before_date = change['updated'].replace(' ', '+')\n                        break\n\n                    try:\n                        reviews = json.loads(reviews.text.lstrip(\")]}\\'\\n\"))\n                    except:\n                        continue\n\n                    change['owner'] = reviews['owner']\n                    for file in comments:\n                        for comm in comments[file]:\n                            if \"author\" in comm:\n                                comm['type'] = \"line\"\n                                comm['file'] = file\n                                if \"labels\" in  reviews and \"Code-Review\" in reviews['labels'] and \"all\" in reviews['labels']['Code-Review']:\n                                    rev = (review for review in reviews['labels']['Code-Review']['all'] if review['_account_id'] == comm['author']['_account_id'])\n                                    try:\n                                        rev = next(rev)\n                                        comm['vote'] = rev['value']\n                                    except StopIteration:\n                                        comm['vote'] = 0\n                                change['comments'].append(comm)\n                    comments = reviews\n                    for comm in comments['messages']:\n                        if \"author\" in comm:\n                            comm['type'] = \"general\"\n                            if \"labels\" in  reviews and \"Code-Review\" in reviews['labels'] and \"all\" in reviews['labels']['Code-Review']:\n                                rev = (review for review in reviews['labels']['Code-Review']['all'] if review['_account_id'] == comm['author']['_account_id'])\n                                try:\n                                    rev = next(rev)\n                                    comm['vote'] = rev['value']\n                                except StopIteration:\n                                    comm['vote'] = 0\n                            change['comments'].append(comm) \n                    entry = {'data': change}\n\n                    yield entry\n            else:\n                time.sleep(600)\n", "entry_point": "fetch"}
{"task_id": "python_11", "code": "\n\n\n\nimport re\n\nRE_KEY = re.compile(r\"[a-zA-Z0-9_]+\")\nRE_KEY_SUFFIX = re.compile(r\"\\[(\\d*)\\]\")\n\n\n\n\nRE_EQ = re.compile(r\"\\s*=((?= \\n))?(?(1)|\\s*)\")\n\nRE_VAL = re.compile(r\"(.*?)((,|\\n|$)|(?=}))\")\nRE_WHITESPACE = re.compile(r\"\\s*\")\n\nclass UnrealNotationParseError(ValueError):\n\tpass\n\nclass Parser():\n\tdef __init__(self, raw_file):\n\t\t\"raw_file: The file's complete contents.\"\n\t\tself.raw_file = raw_file\n\t\tself.pos = 0\n\n\tdef parse(self):\n\t\tself._skip_whitespace()\n\t\tres = {}\n\t\twhile True:\n\t\t\tres.update(self._ident_key_and_parse_kv_pair())\n\t\t\tself._skip_whitespace()\n\t\t\tif self.pos >= len(self.raw_file):\n\t\t\t\tbreak\n\t\treturn res\n\n\tdef _ident_key_and_parse_kv_pair(self):\n\t\t\n\t\tkey_regex = RE_KEY.search(self.raw_file, self.pos)\n\t\tif not key_regex:\n\t\t\traise ValueError()\n\t\tnext_key_name = key_regex[0]\n\t\tself.pos += len(next_key_name)\n\t\tif self.raw_file[self.pos] == \"[\":\n\t\t\tis_list = True\n\t\t\tlen_re = RE_KEY_SUFFIX.search(self.raw_file, self.pos)\n\t\t\tif not len_re:\n\t\t\t\traise UnrealNotationParseError(f\"Expected list length at around {self.pos}\")\n\t\t\tself.pos += len(len_re[0])\n\t\t\tlist_len = int(len_re[1])\n\t\telse:\n\t\t\tis_list = False\n\t\t\tlist_len = -1\n\t\tself._skip_eq() \n\t\t\n\t\treturn self._resolve_element(next_key_name, is_list, list_len)\n\n\tdef _resolve_element(self, key_name, is_list, list_len = None):\n\t\t\n\t\tif self.raw_file[self.pos] == \"{\" and not is_list:\n\t\t\tself.pos += 1\n\t\t\treturn {key_name: self._parse_dict()}\n\t\telif self.raw_file[self.pos] == \"{\" and is_list:\n\t\t\tself.pos += 1\n\t\t\treturn {key_name: self._parse_list(list_len)}\n\t\telse:\n\t\t\t\n\t\t\treturn {key_name: self._parse_simple()}\n\n\tdef _skip_whitespace(self):\n\t\tws_match = RE_WHITESPACE.search(self.raw_file, self.pos)\n\t\tif ws_match:\n\t\t\tself.pos += len(ws_match[0])\n\n\tdef _skip_eq(self):\n\t\teq_search = RE_EQ.search(self.raw_file, self.pos)\n\t\tif not eq_search:\n\t\t\traise UnrealNotationParseError(f\"Expected an equal sign at around {self.pos}\")\n\t\tself.pos += len(eq_search[0])\n\n\tdef _parse_simple(self):\n\t\t\n\t\tval = RE_VAL.search(self.raw_file, self.pos)\n\t\tif not val:\n\t\t\traise UnrealNotationParseError(f\"Expected value at around {self.pos}\")\n\t\tself.pos += len(val[0])\n\t\treturn val[1] \n\n\tdef _parse_dict(self):\n\t\t\n\t\tres = {}\n\t\twhile True:\n\t\t\tself._skip_whitespace()\n\t\t\tres.update(self._ident_key_and_parse_kv_pair())\n\t\t\tself._skip_whitespace()\n\t\t\tif self.raw_file[self.pos] == \"}\":\n\t\t\t\tself.pos += 1\n\t\t\t\tbreak\n\t\t\telif self.pos >= len(self.raw_file) - 1:\n\t\t\t\traise UnrealNotationParseError(\"EOF without closing dict bracket\")\n\t\treturn res\n    \n\tdef _parse_list(self, expected_len):\n\t\t\n\t\tres = [None for _ in range(expected_len)]\n\t\twhile True:\n\t\t\tself._skip_whitespace()\n\t\t\tnext_key = RE_KEY.search(self.raw_file, self.pos)\n\t\t\tif next_key is not None:\n\t\t\t\tnext_key_name = next_key[0]\n\t\t\t\tself.pos += len(next_key_name)\n\t\t\t\tnext_key_idx = RE_KEY_SUFFIX.search(self.raw_file, self.pos)\n\t\t\t\tif not next_key_idx:\n\t\t\t\t\traise UnrealNotationParseError(f\"Index-less list key at around {self.pos}\")\n\t\t\t\tself.pos += len(next_key_idx[0])\n\t\t\t\tnext_key_idx = int(next_key_idx[1])\n\t\t\t\tself._skip_eq()\n\t\t\t\tres[next_key_idx] = next(iter(self._resolve_element(next_key_name, False, -1).values()))\n\n\t\t\tself._skip_whitespace()\n\t\t\tif self.raw_file[self.pos] == \"}\":\n\t\t\t\tself.pos += 1\n\t\t\t\tbreak\n\t\t\telif self.pos >= len(self.raw_file) - 1:\n\t\t\t\traise UnrealNotationParseError(\"EOF without closing list bracket\")\n\t\treturn res\n", "entry_point": "_parse_list"}
{"task_id": "python_12", "code": "import sys\nimport warnings\nfrom functools import wraps\nimport numpy as np\n\nfrom numpy.testing import assert_array_equal\nfrom numpy.testing import assert_array_almost_equal\nfrom numpy.testing import assert_allclose\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_approx_equal\n\nfrom numpy.testing import assert_array_less\n\n__all__ = [\n           \"assert_almost_equal\", \"assert_array_equal\",\n           \"assert_array_almost_equal\", \"assert_array_less\",\n           ]\n\ndef assert_no_warnings(func, *args, **kw):\n    \n    \n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n\n        result = func(*args, **kw)\n        if hasattr(np, 'FutureWarning'):\n            \n            w = [e for e in w\n                 if e.category is not np.VisibleDeprecationWarning]\n\n        if len(w) > 0:\n            raise AssertionError(\"Got warnings when calling %s: [%s]\"\n                                 % (func.__name__,\n                                    ', '.join(str(warning) for warning in w)))\n    return result\n\n\ndef ignore_warnings(obj=None, category=Warning):\n    \n    if isinstance(obj, type) and issubclass(obj, Warning):\n        \n        \n        warning_name = obj.__name__\n        raise ValueError(\n            \"'obj' should be a callable where you want to ignore warnings. \"\n            \"You passed a warning class instead: 'obj={warning_name}'. \"\n            \"If you want to pass a warning class to ignore_warnings, \"\n            \"you should use 'category={warning_name}'\".format(\n                warning_name=warning_name))\n    elif callable(obj):\n        return _IgnoreWarnings(category=category)(obj)\n    else:\n        return _IgnoreWarnings(category=category)\n\nclass _IgnoreWarnings:\n    \n\n    def __init__(self, category):\n        self._record = True\n        self._module = sys.modules['warnings']\n        self._entered = False\n        self.log = []\n        self.category = category\n\n    def __call__(self, fn):\n        \n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", self.category)\n                return fn(*args, **kwargs)\n\n        return wrapper\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        warnings.simplefilter(\"ignore\", self.category)\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n        self.log[:] = []\n\n\ndef assert_warns_message(warning_class, message, func, *args, **kw):\n    \n    \n    with warnings.catch_warnings(record=True) as w:\n        \n        warnings.simplefilter(\"always\")\n        if hasattr(np, 'FutureWarning'):\n            \n            warnings.simplefilter('ignore', np.VisibleDeprecationWarning)\n        \n        result = func(*args, **kw)\n        \n        if not len(w) > 0:\n            raise AssertionError(\"No warning raised when calling %s\"\n                                 % func.__name__)\n\n        found = [issubclass(warning.category, warning_class) for warning in w]\n        if not any(found):\n            raise AssertionError(\"No warning raised for %s with class \"\n                                 \"%s\"\n                                 % (func.__name__, warning_class))\n\n        message_found = False\n        \n        for index in [i for i, x in enumerate(found) if x]:\n            \n            msg = w[index].message  \n            msg = str(msg.args[0] if hasattr(msg, 'args') else msg)\n            if callable(message):  \n                check_in_message = message\n            else:\n                check_in_message = lambda msg: message in msg\n\n            if check_in_message(msg):\n                message_found = True\n                break\n\n        if not message_found:\n            raise AssertionError(\"Did not receive the message you expected \"\n                                 \"('%s') for <%s>, got: '%s'\"\n                                 % (message, func.__name__, msg))\n\n    return result\n", "entry_point": "assert_warns_message"}
{"task_id": "python_13", "code": "from networkx import Graph, connected_components\n\n\ndef intervene(graph,opts):\n\t\n\ttemp = Graph(graph)\n\tcc = list(max(connected_components(graph), key=len))\n\tncc = [n for n in list(graph.nodes()) if n not in cc]\n\ttemp.remove_nodes_from(ncc)\n\n\t\n\teh = opts.initial.max * 0.9\n\tel = opts.initial.min * 0.9\n\textremists = {n: not el < graph.values[\"opinion\"][n] < eh for n in temp.nodes()}\n\n\t\n\ttarget_edges = {}\n\tfor node in temp.nodes():\n\t\t\n\t\tif extremists[node]:\n\t\t\tcontinue\n\t\tneighbors = list(temp[node])\n\t\tmyop = graph.values[\"opinion\"][node]\n\t\textreme_neighbors = [n for n in neighbors if extremists[n]]\n\t\t\n\t\tif len(extreme_neighbors):\n\t\t\tweight = max(abs(1/(myop-graph.values[\"opinion\"][n])) for n in extreme_neighbors)\n\t\t\tweight *= len(extreme_neighbors)/len(neighbors)\n\t\t\tfor ex_n in extreme_neighbors:\n\t\t\t\ttarget_edges[(node,ex_n)] = weight\n\n\t\n\ttargets = sorted(target_edges.items(), key=lambda e: e[1])\n\tif len(targets):\n\t\tedge = targets[-1][0]\n\t\tgraph.remove_edge(*edge)\n\treturn\n", "entry_point": "intervene"}
{"task_id": "python_14", "code": "import numpy as np\nimport scipy.sparse as sps\n\n\ndef _lanczos_m(A, m, nv, rademacher, SV=None):\n    \n    orthtol = 1e-5\n    if type(SV) != np.ndarray:\n        if rademacher:\n            SV = np.sign(np.random.randn(A.shape[0], nv))\n        else:\n            SV = np.random.randn(A.shape[0], nv)  \n    V = np.zeros((SV.shape[0], m, nv))\n    T = np.zeros((nv, m, m))\n\n    np.divide(SV, np.linalg.norm(SV, axis=0), out=SV)  \n    V[:, 0, :] = SV\n\n    w = A.dot(SV)\n    alpha = np.einsum('ij,ij->j', w, SV)\n    w -= alpha[None, :] * SV\n    beta = np.einsum('ij,ij->j', w, w)\n    np.sqrt(beta, beta)\n\n    T[:, 0, 0] = alpha\n    T[:, 0, 1] = beta\n    T[:, 1, 0] = beta\n\n    np.divide(w, beta[None, :], out=w)\n    V[:, 1, :] = w\n    t = np.zeros((m, nv))\n\n    for i in range(1, m):\n        SVold = V[:, i - 1, :]\n        SV = V[:, i, :]\n\n        w = A.dot(SV)  \n        w -= beta[None, :] * SVold  \n        np.einsum('ij,ij->j', w, SV, out=alpha)\n\n        T[:, i, i] = alpha\n\n        if i < m - 1:\n            w -= alpha[None, :] * SV  \n            \n            np.einsum('ijk,ik->jk', V, w, out=t)\n            w -= np.einsum('ijk,jk->ik', V, t)\n            np.einsum('ij,ij->j', w, w, out=beta)\n            np.sqrt(beta, beta)\n            np.divide(w, beta[None, :], out=w)\n\n            T[:, i, i + 1] = beta\n            T[:, i + 1, i] = beta\n\n            \n            innerprod = np.einsum('ijk,ik->jk', V, w)\n            reortho = False\n            for _ in range(100):\n                if not (innerprod > orthtol).sum():\n                    reortho = True\n                    break\n                np.einsum('ijk,ik->jk', V, w, out=t)\n                w -= np.einsum('ijk,jk->ik', V, t)\n                np.divide(w, np.linalg.norm(w, axis=0)[None, :], out=w)\n                innerprod = np.einsum('ijk,ik->jk', V, w)\n\n            V[:, i + 1, :] = w\n\n            if (np.abs(beta) > 1e-6).sum() == 0 or not reortho:\n                break\n    return T, V\n\n\ndef _slq(A, m, niters, rademacher):\n    \n    T, _ = _lanczos_m(A, m, niters, rademacher)\n    eigvals, eigvecs = np.linalg.eigh(T)\n    expeig = np.exp(eigvals)\n    sqeigv1 = np.power(eigvecs[:, 0, :], 2)\n    trace = A.shape[-1] * (expeig * sqeigv1).sum() / niters\n    return trace\n\n\ndef _slq_ts(A, m, niters, ts, rademacher):\n    \n    T, _ = _lanczos_m(A, m, niters, rademacher)\n    eigvals, eigvecs = np.linalg.eigh(T)\n    expeig = np.exp(-np.outer(ts, eigvals)).reshape(ts.shape[0], niters, m)\n    sqeigv1 = np.power(eigvecs[:, 0, :], 2)\n    traces = A.shape[-1] * (expeig * sqeigv1).sum(-1).mean(-1)\n    return traces\n\n\ndef _slq_ts_fs(A, m, niters, ts, rademacher, fs):\n    \n    T, _ = _lanczos_m(A, m, niters, rademacher)\n    eigvals, eigvecs = np.linalg.eigh(T)\n    traces = np.zeros((len(fs), len(ts)))\n    for i, f in enumerate(fs):\n        expeig = f(-np.outer(ts, eigvals)).reshape(ts.shape[0], niters, m)\n        sqeigv1 = np.power(eigvecs[:, 0, :], 2)\n        traces[i, :] = A.shape[-1] * (expeig * sqeigv1).sum(-1).mean(-1)\n    return traces\n\n\ndef slq_red_var(A, m, niters, ts, rademacher):\n    \n    fs = [np.exp, lambda x: x]\n\n    traces = _slq_ts_fs(A, m, niters, ts, rademacher, fs)\n    subee = traces[0, :] - traces[1, :] / np.exp(ts)\n    sub = - ts * A.shape[0] / np.exp(ts)\n    return subee + sub\n", "entry_point": "_lanczos_m"}
{"task_id": "python_15", "code": "import numpy as np\n\nimport torch\nimport torch.nn as nn\n\n\ndef aggr_by_one(model, index_list=None):\n    if not hasattr(model, 'aggr_mask'):\n        model.aggr_mask = dict()\n    if index_list is None:\n        index_list = model.conv_index[1:-1]\n    for ind in index_list:\n        W = model.features[ind].weight.data\n        W_arr = W.cpu().numpy()\n        if ind not in model.aggr_mask.keys():\n            model.aggr_mask[ind] = np.ones_like(W_arr)\n        ch_out, ch_in, ksize, _ = W_arr.shape\n        for i in range(ch_out):\n            for j in range(ch_in):\n                this_kernel = np.squeeze(np.abs(W_arr[i, j, ...]))\n                this_kernel[this_kernel == 0] = 1000.\n                m_ind = np.argmin(this_kernel)\n                m_row = int(m_ind / ksize)\n                m_col = m_ind % ksize\n                W_arr[i, j, m_row, m_col] = 0.\n                model.aggr_mask[ind][i, j, m_row, m_col] = 0.\n        model.features[ind].weight = nn.Parameter(torch.from_numpy(W_arr).cuda())\n\n\ndef mask_aggr_gradient(model, index_list=None):\n    if index_list is None:\n        index_list = model.conv_index[1:-1]  \n\n    for ind in index_list:\n        if ind not in model.aggr_mask.keys():  \n            continue\n        \n        \n        mask = model.aggr_mask[ind]\n        if type(mask) == np.ndarray:\n            mask = torch.from_numpy(mask).cuda()\n        model.features[ind].weight.grad.data = torch.mul(model.features[ind].weight.grad.data, mask)\n\n\ndef aggr_select_layer(model, index, aggr_method='max', mode='cpu', get_mask=False):\n    if not hasattr(model, 'aggr_mask'):\n        model.aggr_mask = dict()\n    W = model.features[index].weight.data\n    if mode == 'cpu':\n        W_arr = W.cpu().numpy()\n        if get_mask:\n            mask = np.zeros_like(W_arr)\n        ch_out, ch_in, ksize, _ = W_arr.shape\n        assert ksize == 3\n        for i in range(ch_out):\n            for j in range(ch_in):\n                m_ind = np.argmax(np.abs(W_arr[i, j, ...]))\n                m_row = int(m_ind / ksize)\n                m_col = m_ind % ksize\n                if aggr_method == 'max':\n                    m_val = W_arr[i, j, m_row, m_col]\n                elif aggr_method == 'sum':\n                    m_val = np.sum(W_arr[i, j, ...])  \n                elif aggr_method == 'weighted':\n                    ss_x = 0.\n                    ss_y = 0.\n                    for k_i in range(ksize):\n                        for k_j in range(ksize):\n                            ss_x += k_i * W_arr[i, j, k_i, k_j]\n                            ss_y += k_j * W_arr[i, j, k_i, k_j]\n                    ss_x /= np.sum(W_arr[i, j, ...])\n                    ss_y /= np.sum(W_arr[i, j, ...])\n                    m_row = int(round(ss_x))\n                    m_col = int(round(ss_y))\n                    m_val = np.sum(W_arr[i, j, ...])\n                else:\n                    raise NotImplementedError\n\n                if get_mask:\n                    mask[i, j, m_row, m_col] = 1.  \n                W_arr[i, j, ...] = np.zeros([ksize, ksize])\n                W_arr[i, j, m_row, m_col] = m_val\n        del model.features[index].weight\n        model.features[index].weight = nn.Parameter(torch.from_numpy(W_arr).cuda())\n        if get_mask:\n            model.aggr_mask[index] = torch.from_numpy(mask).cuda()\n\n    else:\n        raise NotImplementedError\n\n\ndef reset_aggr_mask(model):\n    if hasattr(model, 'aggr_mask'):\n        del model.aggr_mask\n    model.aggr_mask = dict()\n", "entry_point": "aggr_select_layer"}
{"task_id": "python_16", "code": "q   = int(1e9)+7\n\n\ndef binpower(A1,e):\n    n   = len(A1)\n    r   = range(n)\n    A2  = [[0]*n for _ in r]\n    C1  = [[0]*n for _ in r]\n    C2  = [[0]*n for _ in r]\n    for i in r: C1[i][i]=1\n    sA1,sC1 = True,True\n    while True:\n        C = C1 if sC1 else C2\n        A = A1 if sA1 else A2\n        if e &1:\n            T = C2 if sC1 else C1\n            for i in r:\n                for j in r:\n                    T[i][j] = sum([A[i][k]*C[k][j] for k in r])%q\n            sC1 = not sC1\n        e = e>>1\n        if e==0: return T\n        S = A2 if sA1 else A1\n        for i in r:\n            for j in r:\n                S[i][j] = sum([A[i][k]*A[k][j] for k in r])%q \n        sA1 = not sA1\n\nc2i = lambda c: ord(c)-ord('a') if c.islower() else ord(c)-ord('A')+26\n\ndef f(l1,l2):\n    n,m,_ = l1\n    if n==1: return m\n    M  = [[1]*m for _ in range(m)]\n    for s in l2:\n        i = c2i(s[0])\n        j = c2i(s[1])\n        M[i][j]=0\n    return sum([sum(l) for l in binpower(M,n-1)])\n\nl1  =  list(map(int,input().split()))\nl2  = [input() for _ in range(l1[2])]\nprint(f(l1,l2)%q)\n\n", "entry_point": "binpower"}
{"task_id": "python_17", "code": "from typing import List\n\n\nclass Solution:\n    \n    def maximal_rectangle(self, matrix: List[List[str]]) -> int:\n        \n        if not matrix or not matrix[0]:\n            return 0\n        rows, cols = len(matrix), len(matrix[0])\n        heights = [0] * (cols + 1)\n        max_area = 0\n        for row in range(rows):\n            stack = []\n            for col in range(cols + 1):\n                if col < cols and matrix[row][col] == \"1\":\n                    heights[col] += 1\n                else:\n                    heights[col] = 0\n                while stack and heights[stack[-1]] >= heights[col]:\n                    height = heights[stack.pop()]\n                    width = col - stack[-1] - 1 if stack else col\n                    cur_area = height * width\n                    max_area = max(max_area, cur_area)\n                stack.append(col)\n        return max_area\n", "entry_point": "maximal_rectangle"}
{"task_id": "python_18", "code": "\ndef points_to_polynomial(coordinates):\n    try:\n        check = 1\n        more_check = 0\n        d = coordinates[0][0]\n        for j in range(len(coordinates)):\n            if j == 0:\n                continue\n            if d == coordinates[j][0]:\n                more_check += 1\n                solved = \"x=\" + str(coordinates[j][0])\n                if more_check == len(coordinates) - 1:\n                    check = 2\n                    break\n                elif more_check > 0:\n                    check = 3\n                else:\n                    check = 1\n\n        if len(coordinates) == 1 and d == 0:\n            check = 2\n            solved = \"x=0\"\n    except Exception:\n        check = 3\n\n    if check == 1:\n        matrix = []\n        x = len(coordinates)\n\n        for count_of_line in range(x):\n            a = coordinates[count_of_line][0]\n            count_line = [a ** (x - (count_in_line + 1)) for count_in_line in range(x)]\n            matrix.append(count_line)\n\n        vector = [coordinates[count_of_line][1] for count_of_line in range(x)]\n        count = 0\n\n        while count < x:\n            zahlen = 0\n            while zahlen < x:\n                if count == zahlen:\n                    zahlen += 1\n                if zahlen == x:\n                    break\n                bruch = (matrix[zahlen][count]) / (matrix[count][count])\n                for counting_columns, item in enumerate(matrix[count]):\n                    matrix[zahlen][counting_columns] -= item * bruch\n                vector[zahlen] -= vector[count] * bruch\n                zahlen += 1\n            count += 1\n\n        count = 0\n        solution = []\n        while count < x:\n            solution.append(vector[count] / matrix[count][count])\n            count += 1\n\n        solved = \"f(x)=\"\n\n        for count in range(x):\n            remove_e = str(solution[count]).split(\"E\")\n            if len(remove_e) > 1:\n                solution[count] = remove_e[0] + \"*10^\" + remove_e[1]\n            solved += \"x^\" + str(x - (count + 1)) + \"*\" + str(solution[count])\n            if count + 1 != x:\n                solved += \"+\"\n        return solved\n\n    elif check == 2:\n        return solved\n    else:\n        return \"The program cannot work out a fitting polynomial.\"\n\n\nif __name__ == \"__main__\":\n    print(points_to_polynomial([]))\n    print(points_to_polynomial([[]]))\n    print(points_to_polynomial([[1, 0], [2, 0], [3, 0]]))\n", "entry_point": "points_to_polynomial"}
{"task_id": "python_19", "code": "import sys\n\n\ndef merge_items(data, all_items, nocollapse = False, no_complain_others = False):\n    def get_items(prefix, items, values, scale):\n        res = []  \n        total = 0 \n        other = 0 \n        for name, threshold, key_or_items in items:\n            if type(key_or_items) is list:\n                _res, _total, _other = get_items(prefix+name+'-', key_or_items, values, scale)\n                total += _total\n                if _total / scale <= threshold and not nocollapse:\n          \n                    other += _total\n                elif not _res and not nocollapse:\n          \n                    res.append((prefix+name, _total))\n                else:\n          \n                    res += _res\n                    other += _other\n            else:\n                if type(key_or_items) is not tuple:\n                    key_or_items = (key_or_items,)\n                value = 0\n                for key in key_or_items:\n                    if key in values:\n                        value += values[key]\n            \n                        del values[key]\n                total += value\n                if value / scale <= threshold and not nocollapse:\n          \n                    other += value\n                else:\n          \n                    res.append((prefix+name, value))\n        return res, total, other\n\n    results = {}\n    for core, values in data.items():\n        scale = float(sum(values.values())) or 1. \n        res, total, other = get_items('', all_items, values, scale)\n    \n        other += sum(values.values())\n        if other:\n            res.append(('other', other))\n        if values and (not no_complain_others or sum(values.values()) > 0):\n            sys.stderr.write('Also found but not in all_items: %s\\n' % values)\n        results[core] = (res, total, other, scale)\n    return results\n\n\ndef get_names(items, prefix = '', add_prefixes = True, keys = None):\n    names = []\n    for name, threshold, key_or_items in items:\n        if not keys or name in keys:\n            if type(key_or_items) is list:\n                if add_prefixes:\n                    names.append(name) \n                names += get_names(key_or_items, name, add_prefixes)\n            else:\n                if prefix:\n                    names.append(prefix+'-'+name)\n                else:\n                    names.append(name)\n    return names\n", "entry_point": "merge_items"}
{"task_id": "python_20", "code": "\ndef compute_pattern(summ):\n    _first_val = 2\n    _last_val = summ - _first_val\n    comb = []\n    for i in range(summ - 3):\n        comb.append([_first_val, _last_val])\n        _first_val += 1\n        _last_val -= 1\n    return comb\n\n\ndef main():\n    combinations = []\n\n    min_sum = 2\n    first_val = 2\n    last_val = 2\n    summ = 8\n    div = summ // min_sum\n\n    if summ % min_sum == 0:\n        combinations.append([2 for i in range(div)])\n    else:\n        for i in range(div):\n            combinations.append([2 for j in range(div)])\n            combinations[-1][i] = 3\n\n    sum_mod = summ - first_val\n    for i in reversed(range(div)):\n        print(i)\n        while sum_mod > 2:\n            _sum_mod = sum_mod\n            c = []\n            if sum_mod > 3:\n                c.append(compute_pattern(sum_mod))\n                for j in range(1, _sum_mod):\n\n                    while _sum_mod > 3:\n                        _sum_mod -= 1\n                        if _sum_mod > 3:\n                            c.append(compute_pattern(_sum_mod))\n            if len(c) > 0:\n                pass\n            else:\n                combinations.append([first_val, sum_mod])\n            sum_mod -= 1\n            last_val += 1\n            print(c)\n        first_val += 1\n        sum_mod = summ - first_val\n\n    print(combinations, '\\n', len(combinations))\n", "entry_point": "main"}
